{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "eecf2777",
      "metadata": {
        "id": "eecf2777"
      },
      "source": [
        "# CUDA Vector Addition with PyTorch C++ Extensions\n",
        "\n",
        "This notebook demonstrates how to write and compile a custom CUDA kernel for vector addition using PyTorch's C++ extension API. This works in Google Colab without needing external files."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e17e9dc0",
      "metadata": {
        "id": "e17e9dc0"
      },
      "source": [
        "## 1. Check CUDA Availability and Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd7f052e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd7f052e",
        "outputId": "de7c8018-7393-42f8-e4ca-324ef4fa10fb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import sys\n",
        "\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"WARNING: CUDA is not available. This notebook requires GPU runtime.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8b1d705",
      "metadata": {
        "id": "b8b1d705"
      },
      "source": [
        "## 2. Write CUDA Kernel Code\n",
        "\n",
        "We'll create the CUDA kernel file directly in the notebook using the `%%writefile` magic command."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aM0RerLyA5u2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aM0RerLyA5u2",
        "outputId": "0d331091-87f0-4e09-cfa7-7972bce756cc"
      },
      "outputs": [],
      "source": [
        "pip install ninja"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5c8bf8d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5c8bf8d",
        "outputId": "fcdb190d-914a-42da-b9c9-2d0cf00da869"
      },
      "outputs": [],
      "source": [
        "%%writefile addition.cu\n",
        "#include <torch/extension.h>\n",
        "\n",
        "#define CHECK_CUDA(x) TORCH_CHECK(x.device().is_cuda(), #x \" must be a CUDA tensor\")\n",
        "#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x \" must be contiguous\")\n",
        "#define CHECK_INPUT(x) \\\n",
        "  CHECK_CUDA(x); \\\n",
        "  CHECK_CONTIGUOUS(x)\n",
        "\n",
        "__global__ void add_kernel(const float *input1, const float *input2, float *output, int size) {\n",
        "  const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  if (idx < size)\n",
        "    output[idx] = input1[idx] + input2[idx];\n",
        "}\n",
        "\n",
        "torch::Tensor add(torch::Tensor input1, torch::Tensor input2) {\n",
        "  CHECK_INPUT(input1);\n",
        "  CHECK_INPUT(input2);\n",
        "  int size = input1.numel();\n",
        "  TORCH_CHECK(size == input2.numel(), \"input1 and input2 must have the same size\");\n",
        "  torch::Tensor output = torch::empty(size, input1.options());\n",
        "\n",
        "  int n_threads = 256;\n",
        "  int n_blocks = (size + n_threads - 1) / n_threads;\n",
        "  add_kernel<<<n_blocks, n_threads>>>(input1.data_ptr<float>(), input2.data_ptr<float>(), output.data_ptr<float>(), size);\n",
        "\n",
        "  return output;\n",
        "}\n",
        "\n",
        "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
        "  m.def(\"add\", &add, \"Add two vectors\");\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8308dc1a",
      "metadata": {
        "id": "8308dc1a"
      },
      "source": [
        "## 3. Compile CUDA Extension\n",
        "\n",
        "Now we'll compile the CUDA extension using PyTorch's JIT compiler. This may take a minute or two."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd8840a4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd8840a4",
        "outputId": "dce0dabd-a66c-495a-ee7c-f105bf016b7a"
      },
      "outputs": [],
      "source": [
        "from torch.utils.cpp_extension import load\n",
        "\n",
        "print(\"Compiling CUDA extension... This may take a few minutes on first run.\")\n",
        "module = load(\n",
        "    name=\"vector_add_cuda\",\n",
        "    sources=[\"addition.cu\"],\n",
        "    extra_cuda_cflags=[\"-O3\"],\n",
        "    # Adding extra C++ compiler flags for potential compatibility issues\n",
        "    # and to potentially provide more verbose compilation output.\n",
        "    extra_cflags=[\"-std=c++17\", \"-D_GLIBCXX_USE_CXX11_ABI=0\"],\n",
        "    verbose=True,\n",
        ")\n",
        "print(\"\\n✓ Compilation successful!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7dda11e2",
      "metadata": {
        "id": "7dda11e2"
      },
      "source": [
        "## 4. Test Vector Addition\n",
        "\n",
        "Let's create some test vectors and run our custom CUDA kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9077bf6c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9077bf6c",
        "outputId": "a68a756e-a89e-47de-b971-c3300af5410f"
      },
      "outputs": [],
      "source": [
        "# Create random input tensors on GPU\n",
        "size = 1_000_000\n",
        "input1 = torch.randn(size, device=\"cuda\")\n",
        "input2 = torch.randn(size, device=\"cuda\")\n",
        "\n",
        "print(f\"Input tensor 1 shape: {input1.shape}\")\n",
        "print(f\"Input tensor 2 shape: {input2.shape}\")\n",
        "print(f\"Device: {input1.device}\")\n",
        "\n",
        "# Run our custom CUDA kernel\n",
        "output_custom = module.add(input1, input2)\n",
        "\n",
        "print(f\"\\nOutput shape: {output_custom.shape}\")\n",
        "print(f\"First 10 elements of result: {output_custom[:10]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "726e3217",
      "metadata": {
        "id": "726e3217"
      },
      "source": [
        "## 5. Verify Results\n",
        "\n",
        "Compare our custom CUDA kernel output with PyTorch's built-in addition to verify correctness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "694d39f4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "694d39f4",
        "outputId": "e58be772-a3c7-4682-b6ff-555a6cdf99e1"
      },
      "outputs": [],
      "source": [
        "# Compute expected result using PyTorch\n",
        "output_expected = input1 + input2\n",
        "\n",
        "# Verify correctness\n",
        "try:\n",
        "    torch.testing.assert_close(output_custom, output_expected)\n",
        "    print(\"✓ SUCCESS: Custom CUDA kernel produces correct results!\")\n",
        "    print(f\"  Maximum difference: {(output_custom - output_expected).abs().max().item():.2e}\")\n",
        "except AssertionError as e:\n",
        "    print(\"✗ FAILED: Results don't match!\")\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f6bbd6a",
      "metadata": {
        "id": "2f6bbd6a"
      },
      "source": [
        "## 6. Benchmark Performance (Optional)\n",
        "\n",
        "Let's compare the performance of our custom kernel vs PyTorch's built-in addition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8759997e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8759997e",
        "outputId": "ae78c454-b652-4988-fb0f-6b7eca588dbf"
      },
      "outputs": [],
      "source": [
        "# Warmup\n",
        "for _ in range(10):\n",
        "    _ = module.add(input1, input2)\n",
        "    _ = input1 + input2\n",
        "torch.cuda.synchronize()\n",
        "\n",
        "# Benchmark custom kernel using CUDA events\n",
        "n_runs = 1000\n",
        "\n",
        "start_event = torch.cuda.Event(enable_timing=True)\n",
        "end_event = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "start_event.record()\n",
        "for _ in range(n_runs):\n",
        "    _ = module.add(input1, input2)\n",
        "end_event.record()\n",
        "torch.cuda.synchronize()\n",
        "custom_time = start_event.elapsed_time(end_event) / n_runs  # milliseconds\n",
        "\n",
        "# Benchmark PyTorch\n",
        "start_event.record()\n",
        "for _ in range(n_runs):\n",
        "    _ = input1 + input2\n",
        "end_event.record()\n",
        "torch.cuda.synchronize()\n",
        "pytorch_time = start_event.elapsed_time(end_event) / n_runs  # milliseconds\n",
        "\n",
        "print(f\"Vector size: {size:,} elements\")\n",
        "print(f\"Custom CUDA kernel: {custom_time:.4f} ms\")\n",
        "print(f\"PyTorch built-in:   {pytorch_time:.4f} ms\")\n",
        "print(f\"Speedup: {pytorch_time/custom_time:.2f}x {'(custom is faster)' if custom_time < pytorch_time else '(PyTorch is faster)'}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
